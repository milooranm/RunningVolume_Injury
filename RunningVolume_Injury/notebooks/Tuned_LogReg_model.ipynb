{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a260027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.683, Accuracy: 0.678, Recall(most important): 0.560\n",
      "ROC AUC: 0.684, Accuracy: 0.695, Recall(most important): 0.600\n",
      "ROC AUC: 0.696, Accuracy: 0.689, Recall(most important): 0.600\n",
      "ROC AUC: 0.677, Accuracy: 0.704, Recall(most important): 0.560\n",
      "ROC AUC: 0.696, Accuracy: 0.681, Recall(most important): 0.640\n",
      "(0.6780551905387647, 0.56, 0.6832297929191717)\n",
      "(0.6948094612352168, 0.6, 0.6843019372077488)\n",
      "(0.6892247043363995, 0.6, 0.6957181028724115)\n",
      "(0.7040078843626807, 0.56, 0.6769104876419505)\n",
      "(0.681011826544021, 0.64, 0.6956112224448898)\n",
      "Mean Accuracy: 0.689\n",
      "Mean Recall: 0.59\n",
      "Mean ROC AUC: 0.687\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "#import naive bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def normalize2(row, mean_df, std_df, athlete_id):\n",
    "    mu = mean_df.loc[athlete_id]\n",
    "    su = std_df.loc[athlete_id]\n",
    "    z = (row - mu)/su\n",
    "    return z\n",
    "\n",
    "def getMeanStd(data):\n",
    "    mean = data[data['injury'] == 0].groupby('Athlete ID').mean()\n",
    "    std = data[data['injury'] == 0].groupby('Athlete ID').std()\n",
    "    std.replace(to_replace=0.0, value=0.01, inplace=True)\n",
    "    return mean, std\n",
    "\n",
    "def getBalancedSubset(X_train, samplesPerClass):\n",
    "    healthySet   = pd.DataFrame()\n",
    "    unhealthySet = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    stats = pd.DataFrame(X_train[['Athlete ID','injury']].groupby(['Athlete ID','injury']).size().reset_index(name='counts'))\n",
    "    stats = pd.DataFrame(stats[['Athlete ID']].groupby(['Athlete ID']).size().reset_index(name='counts'))\n",
    "    stats.drop(stats[stats['counts'] < 2].index, inplace=True)\n",
    "    athleteList = stats['Athlete ID'].unique()\n",
    "\n",
    "    samplesPerAthlete = int(np.floor(samplesPerClass) / len(athleteList))\n",
    "\n",
    "    for athlete in athleteList:\n",
    "        if unhealthySet.empty:\n",
    "            unhealthySet = X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 0)].sample(samplesPerAthlete, replace=True)\n",
    "        else:\n",
    "            unhealthySet = pd.concat([unhealthySet, X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 0)].sample(samplesPerAthlete,replace=True)], ignore_index=True)\n",
    "        if healthySet.empty:\n",
    "            healthySet = X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 1)].sample(samplesPerAthlete, replace=True)\n",
    "        else:\n",
    "            healthySet = pd.concat([healthySet, X_train[(X_train['Athlete ID'] == athlete) & (X_train['injury'] == 1)].sample(samplesPerAthlete,replace=True)], ignore_index=True)\n",
    "\n",
    "\n",
    "    balancedSet = pd.concat([healthySet, unhealthySet], ignore_index=True)\n",
    "    return balancedSet\n",
    "\n",
    "\n",
    "def preparedata(df,test_athletes):\n",
    "\n",
    "    X_test_original = df[df['Athlete ID'].isin(test_athletes)].copy() # Keep a copy for normalization\n",
    "    X_train_original = df[~df['Athlete ID'].isin(test_athletes)].copy() # Keep a copy\n",
    "\n",
    "    X_train_means, X_train_std = getMeanStd(X_train_original)\n",
    "    X_test_means, X_test_std = getMeanStd(X_test_original)\n",
    "    X_train_balanced = getBalancedSubset(X_train_original.copy(), 2048)\n",
    "    \n",
    "    # Set target variable for testing and training\n",
    "    y_train = X_train_balanced['injury']\n",
    "    y_test = X_test_original['injury']\n",
    "\n",
    "    # Apply normalization to the balanced training data\n",
    "    X_train_norm = X_train_balanced.apply(lambda x: normalize2(x, X_train_means, X_train_std, x['Athlete ID']), axis=1)\n",
    "    X_train_norm = X_train_norm.drop(columns=['injury', 'Date', 'Athlete ID'], errors='ignore')\n",
    "\n",
    "    # Apply normalization to the test data using the testing statistics\n",
    "    # Note this is a source of data leakage! but the alternative is not feasible\n",
    "    X_test_norm = X_test_original.apply(lambda x: normalize2(x, X_test_means, X_test_std, x['Athlete ID']), axis=1)\n",
    "    X_test_norm = X_test_norm.drop(columns=['injury', 'Date', 'Athlete ID'], errors='ignore')\n",
    "\n",
    "\n",
    "    return y_train, y_test, X_train_norm, X_test_norm\n",
    "\n",
    "def train_model(X_train, y_train, **params):\n",
    "    # model = XGBClassifier()\n",
    "    model = LogisticRegression(**params, max_iter=500, class_weight='balanced')\n",
    "    # model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def vis_and_eval(model, y_true, X, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, model.predict_proba(X)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall(most important): {recall:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(3,2))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # print ROC curve and AUC\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.plot(fpr, tpr, label='ROC Curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return (accuracy,recall,roc_auc)\n",
    "\n",
    "def eval(model, y_true, X, y_pred, **params):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, model.predict_proba(X)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}, Accuracy: {accuracy:.3f}, Recall(most important): {recall:.3f}\")\n",
    "   \n",
    "\n",
    "    return (accuracy,recall,roc_auc)\n",
    "\n",
    "def run_exps(df,test_set, n =5, **params):\n",
    "    all_results = []\n",
    "    for i in range(n):\n",
    "        y_train, y_test, X_train, X_test = preparedata(df, test_set)\n",
    "        model = train_model(X_train, y_train,**params)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results = eval(model, y_test, X_test, y_pred)\n",
    "        all_results.append(results)\n",
    "    _ =[print(i) for i in all_results]\n",
    "    # rewrite three lines above as f strings with 3 decimal places\n",
    "    print(f\"Mean Accuracy: {np.mean([x[0] for x in all_results]):.3f}\")\n",
    "    print(f\"Mean Recall: {np.mean([x[1] for x in all_results]):.2f}\")\n",
    "    print(f\"Mean ROC AUC: {np.mean([x[2] for x in all_results]):.3f}\")\n",
    "    \n",
    "    \n",
    "def main():\n",
    "\n",
    "    dfday = pd.read_csv('C:/Users/milo/Desktop/publicprojectsMilo/RunningVolume_Injury/data/raw/day_approach.csv')\n",
    "    dfday.drop(list(dfday.filter(regex = 'perceived|sprinting|strength')), axis = 1, inplace = True)\n",
    "    athletes = sorted(list(dfday['Athlete ID'].unique()))\n",
    "    test_athletes = athletes[len(athletes) - 10:]\n",
    "    run_exps(dfday, test_athletes, n = 5, C=0.01, penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "    '''\n",
    "    print(\"-\" * 50)  # Separator for better readability\n",
    "    C_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "    l1_ratio_values = [0.2, 0.5, 0.8]\n",
    "    for C in C_values:\n",
    "        for l1_ratio in l1_ratio_values:\n",
    "            print(f\"Running experiments with C={C}, l1_ratio={l1_ratio}, penalty='elasticnet', solver='saga', class_weight='balanced'\")\n",
    "            run_exps(dfday, test_athletes, n = 3, C=C, penalty='elasticnet', solver='saga', l1_ratio=l1_ratio)\n",
    "            print(\"-\" * 50)  # Separator for better readability\n",
    "   '''\n",
    "   \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7a09",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "So, I have code that produces fairly well functioning models. I would like to evaluate the data for different testing sets, so I'm going to try to create different training and testing splits for the data. the resulting functionn is below. my results produced quite poor recall, and when outputting the traing and testing splits they were found to be incredibly uneven. I could revisit this, but I would have to really significantly redesign my method for . \n",
    "```python\n",
    "def run_exps(df,athletes, n =5):\n",
    "    all_results = []\n",
    "    for i in range(n):\n",
    "        test_athletes = np.random.choice(athletes, size=10, replace=False)\n",
    "        \n",
    "        y_train, y_test, X_train, X_test = preparedata(df, test_athletes)\n",
    "        \n",
    "        # print number of count of +ve and -ve samples in the training set\n",
    "        print(\"Training set counts: \", y_train[y_train==1].value_counts())\n",
    "        print(\"out of Training set counts: \", y_train.value_counts())\n",
    "        print(\"Testing set counts: \", y_test[y_test==1].value_counts())\n",
    "        print(\"out of Testing set counts: \", y_test.value_counts())\n",
    "        model = train_model(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results = eval(model, y_test, X_test, y_pred)\n",
    "        all_results.append(results)\n",
    "    _ =[print(i) for i in all_results]\n",
    "    print(\"Mean Accuracy: \", np.mean([x[0] for x in all_results]))\n",
    "    print(\"Mean Recall: \", np.mean([x[1] for x in all_results]))\n",
    "    print(\"Mean ROC AUC: \", np.mean([x[2] for x in all_results]))\n",
    "    \n",
    "    \n",
    "def main():\n",
    "\n",
    "    dfday = pd.read_csv('C:/Users/milo/Desktop/publicprojectsMilo/RunningVolume_Injury/data/raw/day_approach.csv')\n",
    "    dfday.drop(list(dfday.filter(regex = 'perceived|sprinting|strength')), axis = 1, inplace = True)\n",
    "    athletes = sorted(list(dfday['Athlete ID'].unique()))\n",
    "    # test_athletes = athletes[len(athletes) - 10:]\n",
    "    run_exps(dfday, athletes, n = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bf9e9",
   "metadata": {},
   "source": [
    "I'm going to try and apply some tuning to the model now, and then will selevt these values for the final model based on what seems to work best\n",
    "\n",
    "surprisingly, it seems tuning affects almost nothing about the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0106d0",
   "metadata": {},
   "source": [
    "Next step is selecting a model to save, saving it, and then creating a pipeline to apply it to personal user training data, and visualise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5a2e1",
   "metadata": {},
   "source": [
    "Then, can work on updating the current pipeline to transform both user data and pipeline data to a longer timeframe, including volume from 1 week or two weeks prior as well as from the past 7 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
